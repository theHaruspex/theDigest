<h1>
<meta charset="utf-8">
 10 Important Lessons We Learned from the 2010s
</h1>
<h2>
</h2>
<i>
 Published October 21, 2020 on Mark Manson: Life Advice that Doesn't Suck.
</i>
<p>
 <span class="stag-dropcap stag-dropcap--squared" style="font-size:125px;line-height:125px;width:125px;height:125px;">
  A
 </span>
 h, yes, the time has come! A time that only occurs a few times in our adult lives. A time that is completely arbitrary and whose importance is invented for the sake of writing clickbaity headlines like this one.
</p>
<p>
 That’s right, it’s the end of the decade, motherfuckers.
 <a class="fn-ref-mark" href="#footnote-1" id="refmark-1">
  <sup>
   1
  </sup>
 </a>
</p>
<p>
 It’s one of those special times when writers and journalists who are whores to the constant internet content cycle get together and decide what the “bests” and “worsts” of the past ten years are. Then they can pretend to sound profound and insightful for a few minutes. I may or may not be one of those whores. You decide.
</p>
<p>
 Because I put together the 10 most important lessons we’ve learned this decade. Why 10 lessons? Because it’s a decade, dumbass! Continuity!
</p>
<p>
 Let’s dive in.
</p>
<p>
 At the beginning of the decade, we were still awash in the idealistic visions of Zuckerbergism — that making the world more connected was an inherently Good Thing. That through technology, we could bring people together, to help them become more tolerant of their differences, that we’d all unite as a species under one great unified banner of funny memes.
</p>
<p>
 <img alt="" class="aligncenter size-full wp-image-65640" data-src="https://markmanson.net/wp-content/uploads/2019/12/zuckerberg.jpg" height="495" loading="lazy" sizes="(max-width: 735px) 100vw, 735px" srcset="https://markmanson.net/wp-content/uploads/2019/12/zuckerberg.jpg 735w, https://markmanson.net/wp-content/uploads/2019/12/zuckerberg-300x202.jpg 300w, https://markmanson.net/wp-content/uploads/2019/12/zuckerberg-640x431.jpg 640w" width="735"/>
</p>
<p>
 On the balance, connecting the world is probably net-positive. But in our heady idealism, we lost track of the potential
 <a href="/the-world-is-fucked" rel="noopener noreferrer" target="_blank">
  costs and side effects
 </a>
 . The decade started with the excitement of democratic movements arising in the most unlikely places around the world—and it ended with decidedly anti-democratic movements also showing up in some of the most unlikely places around the world… And social media somehow managed to be the lifeblood of both of them.
</p>
<p>
 Sadly, it seems that the more people became aware of all the different points of view in the world, the more they wished those alternative views didn’t exist. This odd rise of
 <a href="/outrage" rel="noopener noreferrer" target="_blank">
  intolerance towards opposing views
 </a>
 has spread all across the world and across the political spectrum. No one seems to be immune.
</p>
<p>
 We enter the 2020s with an ambivalent relationship with social media and the flood of information. Whereas I think we began the decade with irrational optimism about our new technology, we are closing the decade with an irrational pessimism about it. The data that social media causes anxiety and depression is weak, at best.
 <a class="fn-ref-mark" href="#footnote-2" id="refmark-2">
  <sup>
   2
  </sup>
 </a>
 Research shows that fake news is losing its influence and people are smartening up about what they’re reading.
 <a class="fn-ref-mark" href="#footnote-3" id="refmark-3">
  <sup>
   3
  </sup>
 </a>
 Despite the apparent dumbing down of the population, we’re actually reading more books than ever before.
 <a class="fn-ref-mark" href="#footnote-4" id="refmark-4">
  <sup>
   4
  </sup>
 </a>
</p>
<p>
 Historically, the introduction of any new form of media causes a lot of disruption in the social order. This has usually led to bloody, protracted wars. But today, for all of the millions of angry tweets, nothing catastrophic has happened.
 <a class="fn-ref-mark" href="#footnote-5" id="refmark-5">
  <sup>
   5
  </sup>
 </a>
</p>
<p>
 I am optimistic for the simple reason that social media allows us to become aware of the effects of its own use in real time. Back in the day, we didn’t realize how badly the Nazis and communists were abusing radio until they had murdered a couple million people. Today, the second anyone does
 <em>
  anything
 </em>
 remotely wrong, half the planet already knows about it.
</p>
<p>
 And while this constant judgment may be infuriating and create more day-to-day stress and frustration, it is probably more self-corrective than previous forms of media, and therefore good for the world in the long-run. The trick is that each one of us has to
 <a href="/fear-of-missing-out" rel="noopener noreferrer" target="_blank">
  learn to manage our relationship with it
 </a>
 .
</p>
<p>
 I think generations from now, we will look back at the 2010s as the point in history where we finally reached attention saturation as a society. What I mean by that is we just lived through the first time in history where the amount of stimulation available to us vastly outstrips our ability to indulge in it.
 <a class="fn-ref-mark" href="#footnote-6" id="refmark-6">
  <sup>
   6
  </sup>
 </a>
</p>
<p>
 This creates a new and unique set of problems. For example, it’s more important than ever before to develop the skill of consciously choosing what deserves our attention and what does not. We must
 <a href="/self-discipline" rel="noopener noreferrer" target="_blank">
  develop discipline
 </a>
 in not getting carried away by our emotional reactions to news. The same way we bargain shop for the best deal on a new laptop or a pair of khakis, we must now be stingy with how we invest our time and attention. It’s taken us awhile, but we’re now figuring that out.
</p>
<p>
 The television age trained us to be docile and receptive. “Show me the shiny funny things, oh, glorious fun box.” But the internet requires us to be active participants in our own consumption. Taking responsibility for that consumption—and managing ourselves when we over-indulge on that consumption—is a difficult and never-ending task. Just this year, in a popular article, I compared it to dieting, even going so far as to call it “
 <a href="/attention-diet" rel="noopener noreferrer" target="_blank">
  The Attention Diet
 </a>
 .”
</p>
<p>
 Similarly, a slew of books and products built around attention management have come out in the past year or two.
 <a class="fn-ref-mark" href="#footnote-7" id="refmark-7">
  <sup>
   7
  </sup>
 </a>
 I imagine this will become a big space in the 2020s, like a smaller version of the fitness and nutrition industries. Like our health, attentional nutrition will be something we’re all expected to maintain and improve to maximize our personal health and well-being.
</p>
<p>
 In 2013, the economist and blogger
 <a href="http://marginalrevolution.com" rel="noopener noreferrer" target="_blank">
  Tyler Cowen
 </a>
 wrote a book called
 <a href="https://amzn.to/2MBW6g2" rel="noopener noreferrer" target="_blank">
  <i>
   Average is Over
  </i>
 </a>
 . Cowen’s argument was simple: the economic realities of 21st-century technology are more likely to produce winner-take-all outcomes than in the past.
</p>
<p>
 He covers the usual suspects for why this happens—software’s ability to scale quickly, monopolistic forces in certain markets, the advantages of elite networks, etc. But he also points out an incredibly counterintuitive reason: information.
</p>
<p>
 When there’s limited information, the people who are best at learning that information and processing it well (i.e., the smartest or most hard-working) will have a limited advantage. Everyone has the same information, but some people use it better than others.
</p>
<p>
 But when information becomes virtually infinite, then the advantages of learning and processing information in impactful ways grow exponentially. Not only are you using the same information better, but you’re able to discover, comprehend, and synthesize far more information than others and do it at a far faster rate.
</p>
<p>
 This ability to
 <a href="/lifelong-learning" rel="noopener noreferrer" target="_blank">
  learn quickly and self-sufficiently
 </a>
 has a compounding effect as well, driving an even greater division in results. The more you learn now, the better you will be able to learn and process new experiences in the future. Therefore, if you develop the ability to learn well—that is, if you
 <a href="/how-to-become-a-better-learner" rel="noopener noreferrer" target="_blank">
  learn how to learn
 </a>
 —you have more opportunities than ever before to accrue huge, compounding results.
</p>
<p>
 Psychologists have understood for a long time that we don’t naturally form beliefs based on truth. Instead, we form our beliefs in order to maintain our sense of identity and security in the world.
 <a class="fn-ref-mark" href="#footnote-8" id="refmark-8">
  <sup>
   8
  </sup>
 </a>
</p>
<p>
 Even as recently as the late 2000s, there was a naive belief that if people were only properly informed about what was going on in the world, they would formulate more accurate beliefs and
 <a href="/decision-making">
  make better decisions
 </a>
 . This assumption stubbornly persists today. Media types—especially in politics—think, “If we just give them more information, they’ll change their minds.”
</p>
<p>
 This assumption backfired. People are probably more entrenched in their beliefs today than at any other time in modern history. Which is strange, considering we are constantly exposed to all of the ways in which
 <a href="/wrong-about-everything" rel="noopener noreferrer" target="_blank">
  we are wrong
 </a>
 .
</p>
<p>
 The truth is that coping with the nuance of contradictory ideas or experiences is stressful. It requires energy and effort to sit with those seemingly opposite things that are both true. The psychologist Leon Festinger called this “
 <a href="https://www.simplypsychology.org/cognitive-dissonance.html" rel="noopener noreferrer" target="_blank">
  cognitive dissonance
 </a>
 ” and argued that people would inevitably resolve that dissonance by blindly believing whatever they wanted to believe.
</p>
<p>
 One of the largest cultural trends of the 2010s worldwide has been this trend toward simplicity. It’s reflected in our political discourse, in our news media, and in our entertainment. There’s a reason why Hollywood keeps pumping out eighteen versions of the same Marvel movie. When we watch superhero movies, there’s an obvious right and wrong. You know exactly what’s good in the world and what’s bad. And on top of that, you know that everyone is cheering for the good guys.
</p>
<p>
 It’s a cinematic means of bringing us back to the child-like simplicity that we crave from the world. I’ve written extensively about the current
 <a href="/how-to-grow-up" rel="noopener noreferrer" target="_blank">
  maturity crisis
 </a>
 in the world today, and this aversion to overwhelming amounts of complex information is likely one of the biggest culprits.
 <a class="fn-ref-mark" href="#footnote-9" id="refmark-9">
  <sup>
   9
  </sup>
 </a>
 It’s also why the ability to learn—i.e., the ability to assess and correct your own mistaken beliefs—is only going to become more valuable in the coming years (see lesson above).
</p>
<p>
 Also, speaking of bad superhero movies…
</p>
<p>
 The
 <a href="/how-to-be-patient" rel="noopener noreferrer" target="_blank">
  overwhelming amount of stimulation available today
 </a>
 has distracted us from another important fact: that we’re living in a golden age of art and creativity.
</p>
<p>
 The business model in the art/entertainment world used to be extremely hierarchical. Either you were some random Jane Doe fighting for scraps in the local bar scene, or you were Michael Jackson, building a goddamn amusement park to greater express your complex shades of pedophilia. There was no in-between. You were either a superstar, or you were broke.
</p>
<p>
 Today, there’s almost nothing
 <em>
  but
 </em>
 the middle. I can get on Spotify and immediately discover half a dozen obscure bands that play exactly the kind of weird music that I’m into… AND those obscure bands can at least make a part-time living off of it.
 <a class="fn-ref-mark" href="#footnote-10" id="refmark-10">
  <sup>
   10
  </sup>
 </a>
 I can get online and binge-watch foreign documentaries with little to no effort. Even Hollywood seems to have had a few new ideas for the first time in eight years. 2019 was actually a great year for film!
 <a class="fn-ref-mark" href="#footnote-11" id="refmark-11">
  <sup>
   11
  </sup>
 </a>
</p>
<p>
 People have said for years that television is in a new golden age. Publishing has quietly entered into a new golden age, with record sales for most of the decade and a resurgence of glue-and-paper books. Millions of books have been self-published and you can now
 <a href="/courses/blogging-writing-course/blogging-writing-course-learn-more" rel="noopener noreferrer" target="_blank">
  start a blog
 </a>
 in a matter of seconds, not hours or weeks, like a decade ago.
</p>
<p>
 It is the best time ever to be an artist. If you’re good, it is easier than ever before to get your work out there and acknowledged.
</p>
<p>
 It is also the best time ever to consume art. It’s cheaper and more available than ever before. It’s easier to find new things to experiment with and it’s easier to support the artists than ever before. If you care at all about good art and entertainment, there is no better time to be alive.
</p>
<p>
 In 2011, within a span of only a few months, I wrote
 <a href="/books/models" rel="noopener noreferrer" target="_blank">
  a men’s dating book
 </a>
 extolling the virtues of vulnerability. Then Brene Brown released a viral TED talk about vulnerability. Both unexpectedly became very popular… okay, her TED talk and books became
 <em>
  waaaayy
 </em>
 more popular than mine.
</p>
<p>
 <span class="yCkrLwneFxh52lpYssEUDe7Q39liA4RG0EY9BN6LjRyO1x81ASPJKpVDBfazWdXfFCHvOc0oUSbZd6">
 </span>
</p>
<p>
</p>
<p>
 The message preached by us and others took root: vulnerability is not a form of weakness; in fact, it’s a demonstration of strength.
</p>
<p>
 <a href="/vulnerability-in-relationships" rel="noopener noreferrer" target="_blank">
  Vulnerability
 </a>
 , as it was defined back then, was the unconditional display of one’s thoughts and feelings—the complete and total willingness to be shot down and rejected. At the beginning of the decade, this became a rally cry for greater emotional health and more authentic experiences with the world in general. It was a great message. And I was proud to be one of the many standing behind it.
</p>
<p>
 Then, as Gary Vaynerchuk likes to say,
 <a href="https://www.inc.com/gary-vaynerchuk/askgaryvee-episode-101-marketers-ruin-everything.html" rel="noopener noreferrer" target="_blank">
  marketers ruin everything
 </a>
 .
</p>
<p>
 It didn’t take long for television shows and social media influencers to deploy “vulnerability” as an engagement tactic. Cry a little here. Smear the makeup there. Move the lighting here. Now say something really emotional about your fears and anxieties of never feeling understood or accepted. Good. Perfect. Aaaanndd…
 <em>
  Cut!
 </em>
</p>
<p>
 Today, vulnerability is borderline cliche. It’s like when a politician, unprompted, launches into a story about how her father was a janitor or something—it’s supposed to be relatable and emotional but you can tell that they practiced it about 500 times in front of focus groups and advisors to get the delivery
 <em>
  just right
 </em>
 .
</p>
<p>
 That is… not vulnerability. It looks like vulnerability. It smells like vulnerability. But vulnerability, it is not.
</p>
<p>
 It’s a corrupt and disingenuous form of vulnerability. It’s saying, “Here are the somewhat embarrassing or endearing
 <em>
  facts
 </em>
 about me, but my intentions are totally calculated and designed to elicit empathy.”
</p>
<p>
 Which is pretty much the total opposite of
 <a href="/vulnerability-in-relationships" rel="noopener noreferrer" target="_blank">
  real vulnerability
 </a>
 .
</p>
<p>
 It just goes to show that anything that can be demonstrated to improve one’s status within a culture can be co-opted and corrupted by those who will do anything for status.
</p>
<p>
 In other words:
 <a href="/negative-self-help" rel="noopener noreferrer" target="_blank">
  humans suck
 </a>
 .
</p>
<p>
 Depending who you are and what you want to do, for the first time in at least three generations, it might make sense to skip going to college.
</p>
<p>
 Universities are more expensive than ever before. They arguably offer a worse education than ever before. And they are less necessary for good careers than ever before.
 <a class="fn-ref-mark" href="#footnote-12" id="refmark-12">
  <sup>
   12
  </sup>
 </a>
</p>
<p>
 That’s not to say that college sucks. Far from it. For many people, going into many professions, college still makes sense. Also, if you’re from a less developed country or live in a country where college is largely paid for by the government, then if you get in, it almost certainly makes sense.
</p>
<p>
 But if you live in the US or a number of other developed nations where the cost of university is now exorbitant and the schools have invested most of their resources into wave pools and glowing ping pong tables rather than sane teachers or up-to-date curriculums, you might want to take a stab at making things work on your own. Just a thought.
</p>
<p>
 A lot is made about political polarization and wealth inequality and the rise of populism. What is rarely discussed is the growing rural/urban divide. Just as this decade began, for the first time in world history, more people were living in cities than in small towns or the countryside.
 <a class="fn-ref-mark" href="#footnote-13" id="refmark-13">
  <sup>
   13
  </sup>
 </a>
 In countries all over the world, the populations are clustering in densely populated megalopolises. There, the cost of living is high, the level of education is high, access to amenities and art is high, populations are diverse and cooperative and safe.
</p>
<p>
 The rural parts of the world, including the small towns and communities, are more and more, being left behind, in terms of economics but also in terms of education, infrastructure, and culture.
</p>
<p>
 This stratification of society between the urban elites and the rural populace creates a phenomenon of two nations living within one country.
 <a class="fn-ref-mark" href="#footnote-14" id="refmark-14">
  <sup>
   14
  </sup>
 </a>
 The economic realities of the internet age and service economies demand high concentrations of wealthy, educated people in one place. This concentration then drives new attitudes and beliefs about equity, fairness, and justice that are completely counter to the traditional values and desires for people living in the interiors of their country. As these values diverge, cultural tensions emerge between the two parts of society. Old vs young. Educated vs uneducated. Diversity vs homogeneity.
</p>
<p>
 Welcome to the 21st century… I hope we don’t all blow each other up.
</p>
<p>
 It’s not popular to applaud corporations these days. But in an age of political gridlock, bureaucratic indecision, crumbling infrastructure, and emerging authoritarian tendencies across the planet, Amazon, Apple, et al. may be the only thing holding the world together right now.
</p>
<p>
 Amazon’s logistical efficiency puts every other human entity to shame. Apple’s global supply chain is likely one of the few things that has prevented some sort of aggression with China. Nobody likes oil companies, but if it weren’t for them, we’d all probably be puppet states of Saudi Arabia. Tech companies like Samsung, Facebook, and Google are the ones spearheading internet infrastructure throughout the third world. And the greatest innovations in renewable energies are coming from the private sector.
</p>
<p>
 Yes, the size of these corporations and their power and outsized influence cause problems. But then again, there are associated problems with any organization wielding lots of power. My point is simply that it’s just as easy to overlook the benefits of these behemoths as it is to criticize their faults. But these companies run the planet more than any international governmental organization. And in some ways, that might be a good thing.
</p>
<p>
 In my book
 <a href="https://amzn.to/2spu7sZ" rel="noopener noreferrer" target="_blank">
  <i>
   Everything is F*cked: A Book About Hope
  </i>
 </a>
 , I wrote about how, by most material metrics, today is the best time in world history to be alive. We are safer, healthier, less violent, more tolerant, more educated, and more literate than ever before. We have more economic freedom, more knowledge, more entertainment, and more connectivity than any other time in world history.
 <a class="fn-ref-mark" href="#footnote-15" id="refmark-15">
  <sup>
   15
  </sup>
 </a>
</p>
<p>
 But… this abundance of information and opportunity takes a psychological toll. It makes it more difficult to find meaning and purpose. It creates the struggle to manage your own attention. And it exposes you to more awful things than ever before.
</p>
<p>
 I call this the
 <a href="/the-paradox-of-progress" rel="noopener noreferrer" target="_blank">
  Paradox of Progress
 </a>
 —the better the world gets, the higher our expectations become for the world, and the more we struggle to find
 <a href="/the-meaning-of-life" rel="noopener noreferrer" target="_blank">
  meaning and significance
 </a>
 in our daily life.
</p>
<p>
 Therefore, while today is undoubtedly the best time to be alive, in many ways, it’s not the easiest time to be alive. While the physical struggle for survival has dissipated for most of the planet, the psychological struggle for meaning has multiplied.
</p>
<p>
 What does all of this mean for the 2020s? Well, coming out of a decade so dominated by public pessimism, I’m oddly optimistic. There have been growing pains as we deal with the effects these new technologies have on society. But the more aware we become of our own faculties and inherent limitations and the more context we gain about the world around us, the better things will become.
</p>
<p>
 So here’s to the coming decade. Let it be less confusing and disorienting than this one!
</p>