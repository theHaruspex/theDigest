<h1>
<meta charset="utf-8">
 Don’t Believe Wrong Things
</h1>
<h2>
</h2>
<i>
 Published April 23, 2018 on Put A Number On It!.
</i>
<div class="entry-content">
 <p>
  LessWrong has a reputation for being a place where dry and earnest people write dry and earnest essays with titles like
  <em>
   “Don’t Believe Wrong Things”
  </em>
  . A casual visitor wouldn’t expect it to host lively discussions of
  <a href="http://slatestarcodex.com/2018/03/26/book-review-twelve-rules-for-life/" rel="noopener">
   prophets
  </a>
  , of
  <a href="https://www.lesserwrong.com/posts/tMhEv28KJYWsu6Wdo/kensh" rel="noopener">
   wizards
  </a>
  , and of
  <a href="https://www.lesswrong.com/posts/mELQFMi9egPn5EAjK/my-attempt-to-explain-looking-insight-meditation-and" rel="noopener">
   achieving enlightenment
  </a>
  . And yet, each of the above links does lead to LessWrong, and each post (
  <a href="https://www.lesswrong.com/posts/tLYKdGBgRXcrzEatb/the-jordan-peterson-mask#comments" rel="noopener">
   including mine
  </a>
  ) has more than a hundred comments.
 </p>
 <p>
  The discussion often turns to a debate that rages eternal in the rationalist community:
  <a href="https://www.lesswrong.com/posts/hZud7CxcEqfYTL7YX/epistemic-and-instrumental-tradeoffs" rel="noopener">
   correctness vs. usefulness
  </a>
  . Rationality is about having true beliefs, we are told, but rationalists should also win. Winning, aka instrumental rationality, sure sounds a lot more fun than just believing true things (epistemic rationality). People are tempted to consider it as the primary goal of rationality, with the pursuit of truth being secondary.
 </p>
 <p>
  Mentions of the “useful but incorrect”, which summarizes my take on Peterson, invite
  <a href="https://www.lesswrong.com/posts/tLYKdGBgRXcrzEatb/the-jordan-peterson-mask#idcyYQnYxD4KBRpLt">
   comments like this
  </a>
  :
 </p>
 <blockquote>
  <p>
   A correct epistemological process is likely to assign very low likelihood to the proposition of Christianity being true at some point. Even if Christianity is true, most Christians don’t have good epistemics behind their Christianity; so if there exists an epistemically justifiable argument for ‘being a Christian’, our hypothetical cradle-Christian rationalist is likely to reach the necessary epistemic skill level to see through the Christian apologetics he’s inherited before he discovers it.
  </p>
  <p>
   At which point he starts sleeping in on Sundays; loses the social capital he’s accumulated through church; has a much harder time fitting in with Christian social groups; and cascades updates in ways that are, given the social realities of the United States and similar countries, likely to draw him toward other movements and behavior patterns, some of which are even more harmful than most denominations of Christianity, and away from the anthropological accumulations that correlate with Christianity, some of which may be harmful but some of which may be protecting against harms that aren’t obvious even to those with good epistemics. Oops! Is our rationalist winning?
  </p>
  <p>
   […] epistemic rationality is important because it’s important for instrumental rationality. But the thing we’re interested in is instrumental rationality, not epistemic rationality. If the instrumental benefits of being a Christian outweigh the instrumental harms of being a Christian, it’s instrumentally rational to be a Christian. If Christianity is false and it’s instrumentally rational to be a Christian, epistemic rationality conflicts with instrumental rationality.
  </p>
 </blockquote>
 <p>
  Well, it’s time for a dry and earnest essay (probably overdue after
  <a href="https://putanumonit.com/2018/04/08/i-desire-u-grpfrt-but-i-wont-eat-u/" rel="noopener">
   last week’s grapefruits
  </a>
  ) on the question of instrumental vs. epistemic rationality. I am not breaking any ground that wasn’t previously covered in the Sequences etc., but I believe that this exercise is justified in the spirit of
  <a href="http://slatestarcodex.com/2017/11/02/non-expert-explanation/" rel="noopener">
   non-expert explanation
  </a>
  .
 </p>
 <p>
  I will attempt to:
 </p>
 <ol>
  <li>
   Dissolve a lot of the dichotomy between “useful” and “correct”, via some examples that use “wrong” wrong.
  </li>
  <li>
   Of the dichotomy that remains, position myself firmly on the
   <em>
    correct
   </em>
   side of the debate.
  </li>
  <li>
   Suggest that convincing yourself of something wrong is, in fact, possible and should be guarded vigilantly against.
  </li>
  <li>
   Say some more in
   <a href="https://www.lesswrong.com/posts/wDP4ZWYLNj7MGXWiW/in-praise-of-fake-frameworks" rel="noopener">
    praise of fake frameworks
   </a>
   , and what they mean if they
   <em>
    don’t
   </em>
   mean “believing in false things”.
  </li>
 </ol>
 <hr/>
 <h2>
  Wrong and Less Wrong
 </h2>
 <p>
  What does “truth” mean, for example in the definition of epistemic rationality as “the pursuit of
  <em>
   true
  </em>
  beliefs about the world”? I think that a lot of the apparent conflict between the “useful” and “true” stems from confusion about the latter word that isn’t merely semantic. As exemplars of this confusion, I will use Brian Lui’s posts:
  <a href="https://brianlui.dog/2018/02/15/wrong-models-are-good/" rel="noopener">
   wrong models are good
  </a>
  ,
  <a href="https://brianlui.dog/2018/03/03/correct-models-are-bad/" rel="noopener">
   correct models are bad
  </a>
  , and
  <a href="https://brianlui.dog/2018/03/20/useful-models-are-better-than-correct-models/" rel="noopener">
   useful models are better than correct models
  </a>
  .
 </p>
 <p>
  I have chosen Brian as a foil because:
 </p>
 <ol>
  <li>
   We actually disagree, but both do so in good faith.
  </li>
  <li>
   I asked him if I could, and he said OK.
  </li>
 </ol>
 <p>
  Here are some examples that Brian uses:
 </p>
 <table>
  <tbody>
   <tr>
    <td width="312">
     <strong>
      Correct Model
     </strong>
    </td>
    <td width="312">
     <strong>
      Useful Model
     </strong>
    </td>
   </tr>
   <tr>
    <td width="312">
     Schrödinger’s model
    </td>
    <td width="312">
     Bohr’s atomic model
    </td>
   </tr>
   <tr>
    <td width="312">
     Calorie-in-calorie-out
    </td>
    <td width="312">
     Focus on satiety
    </td>
   </tr>
   <tr>
    <td width="312">
     Big 5 personality traits
    </td>
    <td width="312">
     MBTI
    </td>
   </tr>
   <tr>
    <td width="312">
     Libertarianism is wrong on many things
    </td>
    <td width="312">
     Libertarianism is right on some things
    </td>
   </tr>
   <tr>
    <td width="312">
     Sphere Earth
    </td>
    <td width="312">
     Flat Earth
    </td>
   </tr>
  </tbody>
 </table>
 <p>
  You may be familiar with Asimov’s quote:
 </p>
 <blockquote>
  <p>
   “When people thought the earth was flat, they were wrong. When people thought the earth was spherical, they were wrong. But if you think that thinking the earth is spherical is just as wrong as thinking the earth is flat, then your view is wronger than both of them put together.”
  </p>
 </blockquote>
 <p>
  People often overlook the
  <a href="http://chem.tufts.edu/answersinscience/relativityofwrong.htm" rel="noopener">
   broader context of the quote
  </a>
  . Asimov makes the point that
  <em>
   Flat Earth is actually
  </em>
  <em>
   a very good model
  </em>
  . Other models could posit an Earth with infinitely tall mountains or bottomless trenches, or perhaps an Earth tilted in such a way that walking north-west would always be uphill. A flat Earth model, built on empiricism and logic, is quite an achievement:
 </p>
 <blockquote>
  <p>
   Perhaps it was the appearance of the plain that persuaded the clever Sumerians to accept the generalization that the earth was flat; that if you somehow evened out all the elevations and depressions, you would be left with flatness. Contributing to the notion may have been the fact that stretches of water (ponds and lakes) looked pretty flat on quiet days.
  </p>
 </blockquote>
 <p>
  A model is correct or not in the context of a specific question asked of it, such as
  <em>
   “Will I arrive back home from the east if I keep sailing west?”
  </em>
  The flat Earth model was perfectly fine until that question was asked, and the first transoceanic voyages took place more than 1,000 after Eratosthenes calculated the spherical Earth’s radius with precision.
 </p>
 <p>
  But it’s not just the “wrong” models that are true, the opposite is also the case,
  <a href="https://en.wikipedia.org/wiki/All_models_are_wrong" rel="noopener">
   as famously noticed by George Box
  </a>
  . The Earth’s shape isn’t a sphere. It’s not even a geoid, it changes moment by moment with the tides, plate tectonics, and ants building anthills. Brian’s division of models into the correct and incorrect starts to seem somewhat arbitrary, so what is it based on?
 </p>
 <p>
  Brian considers the
  <a href="https://en.wikipedia.org/wiki/Big_Five_personality_traits" rel="noopener">
   Big 5 personality model
  </a>
  to more “correct” and “scientific” because it was created using
  <a href="https://twitter.com/yashkaf/status/924148672987828224" rel="noopener">
   factor analysis
  </a>
  , while Myers-Briggs is based on Jung’s conceptual theory. But the trappings of science don’t make a theory true, particularly when the science in question has
  <a href="https://putanumonit.com/2017/06/20/the-battle-for-psychology/" rel="noopener">
   a fraught relationship with the truth
  </a>
  . How “scientific” a process was used to generate a model can correlate with its truthfulness, but as a definition it seems to miss the mark entirely.
 </p>
 <p>
  Rationalists usually measure the truth of a model by the
  <a href="https://www.lesswrong.com/posts/a7n8GdKiAZRX86T5A/making-beliefs-pay-rent-in-anticipated-experiences" rel="noopener">
   rent it pays
  </a>
  when it collides with reality. Neither MBTI nor Big 5 does a whole lot of useful prediction, and they’re not even
  <a href="https://medium.com/s/story/the-mtg-color-wheel-c9700a7cf36d" rel="noopener">
   as fun as the MTG color system
  </a>
  . On the other hand, Bohr’s atomic model works for most questions of basic chemistry and even the photoelectric effect.
 </p>
 <p>
  A model is wrong not because it is not precisely quantified (like satiety), or because it wasn’t published in a science journal (like MBTI), or because it has been superseded by a more reductionist model (like Bohr’s atom). It is wrong when it predicts things that don’t happen or prohibits things that do.
 </p>
 <p>
  When a model’s predictions and prohibitions line up with observable reality, the model is
  <strong>
   true
  </strong>
  . When those predictions are easy to make and check, it is
  <strong>
   useful
  </strong>
  .  Calorie-in-calorie-out isn’t very useful on the question of successful dieting because it is so difficult for people to just change their caloric balance as an immediate action. This difficulty doesn’t make this model any more or less correct, it just means that it’s hard to establish its correctness from seeing whether people who try to count calories lose weight or not. In this view truth and usefulness are almost orthogonal: truth is a precondition for usefulness, while some models are so wrong that they are worse than useless.
 </p>
 <hr/>
 <h2 id="jesus">
  Jesus and Gandhi
 </h2>
 <p>
  Usefulness, in the sense of beliefs paying rent, is a narrower concept than
  <em>
   winning
  </em>
  , e.g., making money to pay your actual rent. The comment
  <a href="https://www.lesswrong.com/posts/tLYKdGBgRXcrzEatb/the-jordan-peterson-mask#idcyYQnYxD4KBRpLt" rel="noopener">
   about the lapsed Christian
  </a>
  I quoted talks about instrumental rationality as the pursuit of actually winning in life. So, is the rejection of Christ epistemically rational but instrumentally irrational?
 </p>
 <p>
  First of all, I think that the main mistake the hypothetical apostate is making is
  <a href="https://www.lesswrong.com/posts/EEv9JeuY5xfuDDSgF/flinching-away-from-truth-is-often-about-protecting-the" rel="noopener">
   a bucket error
  </a>
  . In his mind, there is a single variable labeled “Christianity” which contains a boolean value:
  <em>
   True
  </em>
  or
  <em>
   False
  </em>
  . This single variable serves as an answer to many distinct questions, such as:
 </p>
 <ol>
  <li>
   Did Jesus die for my sins?
  </li>
  <li>
   Should I go to church on Sunday?
  </li>
  <li>
   Should I be nice to my Christian friends?
  </li>
 </ol>
 <p>
  There is no reason why all three questions must have the same answer, as demonstrated by my
  <a href="https://putanumonit.com/2016/09/16/plastic-men/" rel="noopener">
   closet-atheist friend who lives
  </a>
  in an Orthodox Jewish community. The rent in the Jewish part of Brooklyn is pretty cheap (winning!) and doesn’t depend on one’s beliefs about revelation. Living a double life is not ideal, and it is somewhat harder to fit in a religious community if you’re a non-believer. But carelessly propagating new beliefs before sorting out the buckets in one’s head is much more dangerous than zoning out during prayer times. Keeping behaviors that correlate with a false belief is very different from installing new beliefs to change one’s behavior.
 </p>
 <p>
  <a href="https://nickbostrom.com/information-hazards.pdf" rel="noopener">
   Information hazards are also a thing
  </a>
  . There are many real things that we wish other people wouldn’t know, and some things that we wouldn’t want to learn ourselves. But avoiding true but dangerous knowledge is also very different than hunting false beliefs.
 </p>
 <p>
  With that said, what if hunting and installing false beliefs is actually justified? A friend of mine who’s a big fan of Jordan Peterson is joking-not-joking about converting to Christianity.  If Christianity provides one with friends, meaning, and protection from harmful ideologies, isn’t it instrumentally rational to convert?
 </p>
 <p>
  There’s a word for this sort of bargain:
  <a href="https://en.wiktionary.org/wiki/Faustian_bargain">
   Faustian
  </a>
  . One should always imagine this spoken by someone with reddish skin, twisty horns, and an expensive suit.
  <em>
   I offer you all this, and all I want in return is a tiny bit of epistemic rationality. What’s it even worth to you?
  </em>
 </p>
 <p>
  Epistemic rationality is worth a lot.
 </p>
 <p>
  It takes a lot of epistemic rationality to tease apart
  <em>
   causation
  </em>
  from the mere
  <em>
   correlation
  </em>
  of religion with its benefits. Perhaps a Christian’s community likes him because consistent beliefs make a person predictable; this benefit wouldn’t extend to a fresh convert. As for meaning and protection from adverse memes, are those provided by Jesus or by the community itself? Or by some confounder like age or geography?
 </p>
 <p>
  A person discerning enough on matters of friendship to judge whether it is the
  <em>
   cause
  </em>
  or the
  <em>
   effect
  </em>
  of Christian belief probably understands friendship well enough to make friends with or without converting. I help run a
  <a href="https://www.lesswrong.com/groups/4ee8NedvMNvoSitzj" rel="noopener">
   weekly meetup of rationalists in New York
  </a>
  . We think a lot about building an active community, and we implement this in practice. We may not provide the full spiritual package of a church, but we also don’t demand a steep price from our members: neither in money, nor in effort, nor in dogma.
 </p>
 <p>
  Perhaps converting
  <em>
   is
  </em>
  the instrumentally optimal thing to do for a young rationalist, but it would require heroic epistemic rationality to know that it is so. And once you have converted, that epistemic rationality is gone forever, along with the ability to reason well about such trade-offs in the future. If you discover a new religion tomorrow that offers ten times the benefits of Christianity, it would be too late: your new belief in the truth of Christianity will prevent you from even considering the option of reconverting to the new religion.
 </p>
 <p>
  This argument is colloquially known as
  <em>
   <a href="https://www.lesswrong.com/posts/Kbm6QnJv9dgWsPHQP/schelling-fences-on-slippery-slopes" rel="noopener">
    The Legend of Murder-Gandhi
   </a>
  </em>
  . Should Gandhi, who abhors violence, take a pill that makes him 99% as reluctant to commit murder for a million dollars? No, because 99%-pacifist Gandhi will not hesitate to take another pill and go to 98%, and then down to 97%, and to 90%,
 </p>
 <blockquote>
  <p>
   <span style="background-color:#e9eff3;color:#4f748e;">
    and so on until he’s rampaging through the streets of Delhi, killing everything in sight.
   </span>
  </p>
 </blockquote>
 <p>
  An exception could be made if Gandhi had a way to commit himself to stopping at 95% pacifism; that’s still pacifist enough that he doesn’t really need to worry about acting violently, yet $5 million richer.
 </p>
 <p>
  But epistemic rationality is a higher-level skill than mere pacifism. It’s the skill that’s necessary not only to assess a single trade-off, but also to understand the dangers of slippery slopes, and the benefits of pre-commitments, and the need for
  <a href="https://arxiv.org/abs/1710.05060" rel="noopener">
   Functional Decision Theory
  </a>
  in a world full of
  <a href="https://www.lesswrong.com/posts/puutBJLWbg2sXpFbu/newcomblike-problems-are-the-norm" rel="noopener">
   Newcomblike problems
  </a>
  . Gandhi who’s perfectly pacifist but doesn’t understand Schelling fences will take the first pill, and all his pacifism will be for naught.
 </p>
 <p>
  Do you think you have enough epistemic rationality to determine when it’s
  <em>
   really
  </em>
  worth sacrificing epistemic rationality for something else? Better to keep increasing your epistemic rationality, just to be sure.
 </p>
 <hr/>
 <h2>
  Flat Moon Society
 </h2>
 <p>
  Is this a moot point, though? It’s not like you can make yourself go to sleep an atheist and wake up a devout Christian tomorrow. Eliezer wrote a
  <a href="https://www.lesswrong.com/sequences/qqFS6Kw5fmPyzkLby" rel="noopener">
   whole sequence
  </a>
  on the inability to self-deceive:
 </p>
 <blockquote>
  <p>
   We do not have such direct control over our beliefs.  You cannot make yourself believe the sky is green by an act of will.  You might be able to
   <a href="https://www.lesswrong.com/lw/i4/belief_in_belief/">
    believe you believed
   </a>
   it—though I have just made that more difficult for you by pointing out the difference.  (You’re welcome!)  You might even
   <em>
    believe
   </em>
   you were happy and self-deceived; but you would not
   <em>
    in fact
   </em>
   be happy and self-deceived.
  </p>
  <p>
   […]
  </p>
  <p>
   You can’t know the consequences of being biased, until you have already debiased yourself.  And then it is too late for self-deception.
  </p>
  <p>
   The other alternative is to choose blindly to remain biased, without any clear idea of the consequences.  This is not second-order rationality.  It is willful stupidity.
  </p>
 </blockquote>
 <p>
  He gives an example of very peculiar Orthodox Jew:
 </p>
 <blockquote>
  <p>
   When this woman was in high school, she thought she was an atheist.  But she decided, at that time, that she should act as if she believed in God.  And then—she told me earnestly—over time, she came to really believe in God.
  </p>
  <p>
   So far as I can tell, she is completely wrong about that.  Always throughout our conversation, she said, over and over, “I
   <em>
    believe
   </em>
   in God”, never once, “There
   <em>
    is
   </em>
   a God.”  When I asked her why she was religious, she never once talked about the consequences of God existing, only about the consequences of believing in God.  Never, “God will help me”, always, “my belief in God helps me”.  When I put to her, “Someone who just wanted the truth and looked at our universe would not even invent God as a hypothesis,” she agreed outright.
  </p>
  <p>
   She hasn’t
   <em>
    actually
   </em>
   deceived herself into believing that God exists or that the Jewish religion is true.  Not even close, so far as I can tell.
  </p>
  <p>
   On the other hand, I think she really
   <em>
    does
   </em>
   believe she has deceived herself.
  </p>
 </blockquote>
 <p>
  But eventually, he admits that believing you
  <em>
   won’t
  </em>
  self-deceive is also somewhat of a self-fulfilling prophecy:
 </p>
 <blockquote>
  <p>
   It may be wise to go around deliberately repeating “I can’t get away with double-thinking!  Deep down, I’ll know it’s not true!  If I know my map has no reason to be correlated with the territory, that means I don’t believe it!”
  </p>
  <p>
   Because that way—if you’re ever tempted to try—the thoughts “But I know this isn’t really true!” and “I can’t fool myself!” will always rise readily to mind; and that way, you will indeed be less likely to fool yourself successfully.  You’re more likely to get, on a gut level, that telling yourself X doesn’t make X true: and therefore, really truly not-X.
  </p>
 </blockquote>
 <p>
  To me the sequence’s message is
  <em>
   “don’t do it!”
  </em>
  rather than
  <em>
   “it’s impossible!”.
  </em>
  If self-deception were impossible, there would be no need for injunctions against it.
 </p>
 <p>
  Self-deception definitely isn’t
  <em>
   easy
  </em>
  . A good friend of mine told me about two guys he knows who are aspiring flat-Earthers. Out of the pure joy of contrarianism, the two have spent countless hours watching flat-Earth apologia on YouTube. So far their yearning for globeless epiphany hasn’t been answered, although they aren’t giving up.
 </p>
 <p>
  A coworker of mine feels that every person should believe in at least one crazy conspiracy theory, and so he says that he convinced himself that the moon landing was faked. It’s hard to tell if he fully believes it, but he probably believes it
  <em>
   somewhat
  </em>
  . His actual beliefs about NASA have changed, not just his beliefs-in-self-deception. Perhaps earlier in life, he would have bet that the moon landing was staged in a movie studio at million-to-one odds, and now he’ll take that bet at 100:1.
 </p>
 <p>
  He is certainly less likely to discount the other opinions of moon landing-skeptics, which leaves him a lot more vulnerable to being convinced of bullshit in the future. And the mere belief-in-belief is still a wrong belief that was created in his mind ex-nihilo. My colleague clearly sacrificed some amount of epistemic rationality, although it’s unclear what he got in return.
 </p>
 <p>
  Self-deception works like deception. False beliefs sneak into your brain the same way
  <a href="https://putanumonit.com/2018/04/08/i-desire-u-grpfrt-but-i-wont-eat-u/" rel="noopener">
   a grapefruit does
  </a>
  .
 </p>
 <ol>
  <li>
   <span style="font-weight:400;">
    First, we hear something stated as fact:
    <em>
     the moon landing was staged
    </em>
    . Our brain’s immediate reaction on a neurological level to a new piece of information is to believe it. Only when propagating the information shows it to be in conflict with prior beliefs is it discarded. But nothing can ever be discarded entirely by our brains, and a small trace remains.
   </span>
  </li>
  <li>
   <span style="font-weight:400;">
    We come across the same information a few more times. Now, the brain recognizes it as familiar, which means that it anchors itself deeper into the brain even if it is disbelieved every time. The traces accumulate. Was the footage of the moon landing really all it seemed?
   </span>
  </li>
  <li>
   <span style="font-weight:400;">
    Perhaps we associate a positive feeling with the belief. Wouldn’t it be cool if the Appolo missions never happened? This means that I can still be the first human on the moon!
   </span>
  </li>
  <li>
   <span style="font-weight:400;">
    Even if we still don’t believe the original lie when questioning it directly, it still occupies some territory in our head. Adjacent beliefs get reinforced through confirmation bias, which in turn reinforces the original lie. If the “landing” was really shot on the moon, why was the flag rippling in the wind? Wait, is the flag actually rippling? We don’t remember, it’s not like we watch moon landing footage every day. But now we believe that the flag was rippling, which reinforces the belief that the moon landing was fake.
   </span>
  </li>
  <li>
   <span style="font-weight:400;">
    We forget where we initially learned the information from. Even if the original claim about the moon fakery was presented as untrue and immediately debunked, we will just remember that we heard somewhere that it was all an elaborate production to fool the Russians. We recall that we used to be really skeptical of the claim once, but it sure feels like a lot of evidence has been pointing that way recently…
   </span>
  </li>
 </ol>
 <p>
  It is easiest to break this chain on step 1 – avoid putting trash into your brain. As an example, I will never read the Trump exposé
  <em>
   Fire and Fury
  </em>
  under any circumstances, and implore my friends to do the same. Practically everyone agrees that the book has ten rumors and made up stories for every single verifiable fact, but if you read the book, you don’t know which is which. If you’re the kind of person who’s already inclined to believe anything and everything about Donald Trump, reading the book will inevitably make you stupider and less informed about the president. And this “kind of person” apparently includes most of the country, because no parody of
  <em>
   Fire and Fury
  </em>
  <a href="https://www.avclub.com/people-are-very-willing-to-believe-that-trump-watches-a-1821815578">
   has been too outlandish to be believed
  </a>
  .
 </p>
 <hr/>
 <h2>
  <span style="font-weight:400;">
   Take the Glasses Off
  </span>
 </h2>
 <p>
  So, what are “
  <a href="https://www.lesswrong.com/posts/wDP4ZWYLNj7MGXWiW/in-praise-of-fake-frameworks" rel="noopener">
   fake frameworks
  </a>
  ” and what do they have to do with all of this?
 </p>
 <blockquote>
  <p>
   I use a lot of fake frameworks — that is, ways of seeing the world that are probably or obviously wrong in some important way.
  </p>
  <p>
   […]
  </p>
  <p>
   Assume the intuition is wrong. It’s fake. And then
   <a href="http://www.quotationspage.com/quote/26277.html">
    use it anyway
   </a>
   .
  </p>
 </blockquote>
 <p>
  It almost sounds as if Val is saying that we should believe in wrong things, but I don’t think that’s the case. Here’s the case.
 </p>
 <p>
  First of all, you should use a safety mechanism when dealing with fake frameworks:
  <a href="https://en.wikipedia.org/wiki/Sandbox_(computer_security)" rel="noopener">
   sandboxing
  </a>
  . This means holding the belief in a separate place where it doesn’t propagate.
 </p>
 <p>
  This is why I talk about wearing a “Peterson mask”, or having Peterson as a voice on your shoulder. The goal is to generate answers to questions like
  <em>
   “What would Peterson tell me to do here? And how would Scott Alexander respond?”
  </em>
  rather than literally replacing your own beliefs with someone else’s.  Answering those questions does require
  <em>
   thinking as Peterson
  </em>
  for a while, but you can build scaffolding that prevents that mode of thinking from taking over.
 </p>
 <p>
  But sandboxing is secondary to the main point of fake frameworks: they’re not about believing new things, they’re about
  <em>
   un-believing things
  </em>
  .
 </p>
 <p>
  A lot of fake frameworks deal with the behavior of large numbers of people: coordination problems are
  <a href="http://slatestarcodex.com/2014/07/30/meditations-on-moloch/" rel="noopener">
   an ancient hungry demon
  </a>
  , the social web forces people
  <a href="https://www.lesserwrong.com/posts/AqbWna2S85pFTsHH4/the-intelligent-social-web" rel="noopener">
   into playing roles
  </a>
  ,
  <a href="https://thezvi.wordpress.com/2017/04/22/against-facebook/" rel="noopener">
   Facebook
  </a>
  is
  <a href="https://thezvi.wordpress.com/2017/09/23/out-to-get-you/" rel="noopener">
   out to get you
  </a>
  . In what sense is
  <em>
   Facebook
  </em>
  out to get you? Facebook is thousands of employees and millions of shareholders pursuing their own interest, not a unified agent with desires.
 </p>
 <p>
  But neither is a person.
 </p>
 <p>
  People minds are made up of a multitude of independent processes,
  <a href="https://www.lesserwrong.com/posts/yex7E6oHXYL93Evq6/book-review-consciousness-explained" rel="noopener">
   conscious and unconscious
  </a>
  , each influencing our interactions with the world. Our single-minded pursuit of genetic fitness has shattered into
  <a href="https://www.lesserwrong.com/posts/cSXZpvqpa9vbGGLtG/thou-art-godshatter" rel="noopener">
   a thousand shards of desire
  </a>
  . Insofar as we have strategic goals such as being out to get someone, we are constantly distracted from them and constantly changing them.
 </p>
 <p>
  The insight of fake frameworks is that
  <em>
   every
  </em>
  framework you use is fake, especially when talking about complicated things like people and societies. “Society” and “person” themselves aren’t ontologically basic entities, just useful abstractions. Useful, but not 100%
  <em>
   true
  </em>
  .
 </p>
 <p>
  And yet, you have to interact with people and societies every day.  You can’t do it without
  <em>
   some
  </em>
  framework of thinking about people; a cocktail party isn’t navigable on the level of quarks or molecules or cells. You have to see human interaction through one pair of glasses or another. The glasses you look through impose some meaning on the raw data of moving shapes and mouth sounds, but that meaning is “fake”: it’s part of the map, not the territory.
 </p>
 <p>
  Once you realize that you’re wearing glasses, it’s hard to forget that fact. You can now safely take the glasses off and replace them with another pair, without confusing what you see through the lenses with what exists on a fundamental level. The process is gradual, peeling away layer after layer of immutable facts that turned out to be interpretations. Every time a layer is peeled away, you have more freedom to play with new frameworks of interpretation to replace it.
 </p>
 <p>
  If you can stand one more visual-based metaphor, the skill of removing glass is also called
  <em>
   Looking
  </em>
  . This art is hard and long and I’m only a novice in it, but I have a general sense of the direction of progress. There seems to be a generalizable skill of Looking and playing with frameworks, as well as domain-specific understanding that is required for Looking in different contexts. Deep curiosity is needed,
  <a href="https://wiki.lesswrong.com/wiki/Virtues_of_rationality">
   and also relinquishment
  </a>
  . It often takes an oblique approach rather than brute force. For example, people report the illusion of a coherent “self” being dispelled by such varied methods as meditation, falling in love, taking LSD, and studying philosophy.
 </p>
 <p>
  Finally, while I can’t claim
  <a href="https://www.lesserwrong.com/posts/tMhEv28KJYWsu6Wdo/kensh#wgb3wu6kQYdCpoehL">
   the benefits that others can
  </a>
  , I think that Looking offers real protection against being infected with wrong beliefs. Looking is internalizing that some of your beliefs about the world are actually interpretations you impose on it. False interpretations are much easier to critically examine and detach from than false beliefs. You end up believing fewer
  <em>
   wrong
  </em>
  things about the world simply because you believe
  <em>
   fewer things
  </em>
  about the world.
 </p>
 <p>
  And if Looking seems beyond reach, believing fewer wrong things is always a good start.
 </p>
 <div class="sharedaddy sd-like-enabled sd-sharing-enabled" id="jp-post-flair">
  <div class="sharedaddy sd-sharing-enabled">
   <div class="robots-nocontent sd-block sd-social sd-social-icon sd-sharing">
    <h3 class="sd-title">
     Share this:
    </h3>
    <div class="sd-content">
     <ul>
      <li class="share-facebook">
       <a class="share-facebook sd-button share-icon no-text" data-shared="sharing-facebook-28643" href="https://putanumonit.com/2018/04/23/dont-believe-wrong-things/?share=facebook" rel="nofollow noopener noreferrer" target="_blank" title="Click to share on Facebook">
        <span>
        </span>
        <span class="sharing-screen-reader-text">
         Click to share on Facebook (Opens in new window)
        </span>
       </a>
      </li>
      <li class="share-twitter">
       <a class="share-twitter sd-button share-icon no-text" data-shared="sharing-twitter-28643" href="https://putanumonit.com/2018/04/23/dont-believe-wrong-things/?share=twitter" rel="nofollow noopener noreferrer" target="_blank" title="Click to share on Twitter">
        <span>
        </span>
        <span class="sharing-screen-reader-text">
         Click to share on Twitter (Opens in new window)
        </span>
       </a>
      </li>
      <li class="share-pocket">
       <a class="share-pocket sd-button share-icon no-text" data-shared="" href="https://putanumonit.com/2018/04/23/dont-believe-wrong-things/?share=pocket" rel="nofollow noopener noreferrer" target="_blank" title="Click to share on Pocket">
        <span>
        </span>
        <span class="sharing-screen-reader-text">
         Click to share on Pocket (Opens in new window)
        </span>
       </a>
      </li>
      <li class="share-reddit">
       <a class="share-reddit sd-button share-icon no-text" data-shared="" href="https://putanumonit.com/2018/04/23/dont-believe-wrong-things/?share=reddit" rel="nofollow noopener noreferrer" target="_blank" title="Click to share on Reddit">
        <span>
        </span>
        <span class="sharing-screen-reader-text">
         Click to share on Reddit (Opens in new window)
        </span>
       </a>
      </li>
      <li class="share-tumblr">
       <a class="share-tumblr sd-button share-icon no-text" data-shared="" href="https://putanumonit.com/2018/04/23/dont-believe-wrong-things/?share=tumblr" rel="nofollow noopener noreferrer" target="_blank" title="Click to share on Tumblr">
        <span>
        </span>
        <span class="sharing-screen-reader-text">
         Click to share on Tumblr (Opens in new window)
        </span>
       </a>
      </li>
      <li class="share-email">
       <a class="share-email sd-button share-icon no-text" data-shared="" href="https://putanumonit.com/2018/04/23/dont-believe-wrong-things/?share=email" rel="nofollow noopener noreferrer" target="_blank" title="Click to email this to a friend">
        <span>
        </span>
        <span class="sharing-screen-reader-text">
         Click to email this to a friend (Opens in new window)
        </span>
       </a>
      </li>
      <li class="share-end">
      </li>
     </ul>
    </div>
   </div>
  </div>
  <div class="sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-unloaded" data-name="like-post-frame-101823629-28643-6010a34632f00" data-src="//widgets.wp.com/likes/index.html?ver=20200826#blog_id=101823629&amp;post_id=28643&amp;origin=putanumonit.wordpress.com&amp;obj_id=101823629-28643-6010a34632f00&amp;domain=putanumonit.com" id="like-post-wrapper-101823629-28643-6010a34632f00">
   <h3 class="sd-title">
    Like this:
   </h3>
   <div class="likes-widget-placeholder post-likes-widget-placeholder" style="height: 55px;">
    <span class="button">
     <span>
      Like
     </span>
    </span>
    <span class="loading">
     Loading...
    </span>
   </div>
   <span class="sd-text-color">
   </span>
   <a class="sd-link-color">
   </a>
  </div>
  <div class="jp-relatedposts" id="jp-relatedposts">
   <h3 class="jp-relatedposts-headline">
    <em>
     Related
    </em>
   </h3>
  </div>
 </div>
</div>